{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5898b4-4b3a-421c-b5ec-be1a941e4727",
   "metadata": {},
   "source": [
    "# 🚀 Final Preprocessing Code using Scikit-Learn Pipelines 🛠️\n",
    "\n",
    "In this section, we'll consolidate everything we've done so far into one final script using **Scikit-Learn Pipelines**. This includes:\n",
    "\n",
    "* 📊 **Creating a stratified test set**\n",
    "* 🔄 **Handling missing values**\n",
    "* 🔢 **Encoding categorical variables**\n",
    "* ⚖️ **Scaling numerical features**\n",
    "* 🔗 **Combining everything using Pipeline and ColumnTransformer**\n",
    "\n",
    "This will ensure **clean**, **modular**, and **reproducible** code — perfect for production and education! 😎\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Code Walkthrough\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.preprocessing import OrdinalEncoder  # Uncomment if you prefer ordinal\n",
    "```\n",
    "\n",
    "### 1. 📥 **Load the Data**\n",
    "\n",
    "First, let's load our dataset:\n",
    "\n",
    "```python\n",
    "housing = pd.read_csv(\"housing.csv\")\n",
    "```\n",
    "\n",
    "### 2. 🎯 **Create a Stratified Test Set based on Income Category**\n",
    "\n",
    "We want to split the data such that income categories are evenly distributed between train and test sets:\n",
    "\n",
    "```python\n",
    "housing[\"income_cat\"] = pd.cut(\n",
    "    housing[\"median_income\"],\n",
    "    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "    labels=[1, 2, 3, 4, 5]\n",
    ")\n",
    "```\n",
    "\n",
    "Next, we'll use **StratifiedShuffleSplit** to create a split based on the income category:\n",
    "\n",
    "```python\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index].drop(\"income_cat\", axis=1)\n",
    "    strat_test_set = housing.loc[test_index].drop(\"income_cat\", axis=1)\n",
    "```\n",
    "\n",
    "### 3. 🧹 **Separate Predictors and Labels**\n",
    "\n",
    "Now, we'll separate out the labels from the features:\n",
    "\n",
    "```python\n",
    "housing_labels = housing[\"median_house_value\"].copy()\n",
    "housing = housing.drop(\"median_house_value\", axis=1)\n",
    "```\n",
    "\n",
    "### 4. 🔠 **Separate Numerical and Categorical Columns**\n",
    "\n",
    "We’ll divide the columns into **numerical** and **categorical**:\n",
    "\n",
    "```python\n",
    "num_attribs = housing.drop(\"ocean_proximity\", axis=1).columns.tolist()\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "```\n",
    "\n",
    "### 5. 🛠️ **Set Up Pipelines**\n",
    "\n",
    "#### ➡️ **Numerical Pipeline**\n",
    "\n",
    "We’ll handle missing values using a **SimpleImputer** (with median strategy) and scale the numerical features:\n",
    "\n",
    "```python\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),  # Handle missing data\n",
    "    (\"scaler\", StandardScaler()),  # Scale numerical features\n",
    "])\n",
    "```\n",
    "\n",
    "#### ➡️ **Categorical Pipeline**\n",
    "\n",
    "For categorical variables, we’ll use **OneHotEncoder**:\n",
    "\n",
    "```python\n",
    "cat_pipeline = Pipeline([\n",
    "    # (\"ordinal\", OrdinalEncoder())  # Use this if you prefer ordinal encoding\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))  # OneHot encoding\n",
    "])\n",
    "```\n",
    "\n",
    "### 6. 🔗 **Full Pipeline with ColumnTransformer**\n",
    "\n",
    "Combine the pipelines into a **ColumnTransformer** to process numerical and categorical features separately:\n",
    "\n",
    "```python\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])\n",
    "```\n",
    "\n",
    "### 7. 🔄 **Transform the Data**\n",
    "\n",
    "Now, we apply the transformation to the training set:\n",
    "\n",
    "```python\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "```\n",
    "\n",
    "The result, `housing_prepared`, is a **NumPy array** that's now ready for training a model!\n",
    "\n",
    "```python\n",
    "# housing_prepared is now a NumPy array ready for training\n",
    "print(housing_prepared.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894af975-2471-42aa-beb5-659ce3681c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
