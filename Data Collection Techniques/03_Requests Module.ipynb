{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdf6c97-5e4a-416d-8750-9166078d2c8c",
   "metadata": {},
   "source": [
    "# üåê Using `requests` Module for Data Collection\n",
    "\n",
    "Today we‚Äôll see how to scrape websites and use the `requests` module to download the **raw HTML** of a webpage.\n",
    "\n",
    "‚úÖ For scraping demos, we can safely use:\n",
    "- üîπ https://quotes.toscrape.com/\n",
    "- üîπ https://books.toscrape.com/\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ What is `requests`?\n",
    "\n",
    "`requests` is a **Python library** used to send HTTP requests easily.\n",
    "\n",
    "- üì• Allows you to **fetch the content** of a webpage programmatically.\n",
    "- üîÑ Commonly used as the **first step before parsing HTML** with BeautifulSoup.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Installing `requests`\n",
    "\n",
    "To install `requests`, run:\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Sending a Basic GET Request\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the HTML content\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "üìù **Key points**:\n",
    "\n",
    "* `url`: The website you want to fetch.\n",
    "* `response.text`: The HTML content of the page as a string.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Checking the Response Status\n",
    "\n",
    "Always check if the request was successful:\n",
    "\n",
    "```python\n",
    "print(response.status_code)\n",
    "```\n",
    "\n",
    "üìä **Common Status Codes**:\n",
    "\n",
    "* ‚úÖ 200: OK (Success)\n",
    "* ‚ùå 404: Not Found\n",
    "* üö´ 403: Forbidden\n",
    "* üí• 500: Internal Server Error\n",
    "\n",
    "üëç **Good practice**:\n",
    "\n",
    "```python\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "else:\n",
    "    print(\"Failed to fetch the page.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Important Response Properties\n",
    "\n",
    "| Property               | Description                             |\n",
    "| ---------------------- | --------------------------------------- |\n",
    "| `response.text`        | HTML content as Unicode text            |\n",
    "| `response.content`     | Raw bytes of the response               |\n",
    "| `response.status_code` | HTTP status code                        |\n",
    "| `response.headers`     | Metadata like content-type, server info |\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Adding Headers to Mimic a Browser\n",
    "\n",
    "Sometimes websites block automated requests. üõ°Ô∏è\n",
    "\n",
    "Adding a **User-Agent** header helps the request look like it's coming from a real browser:\n",
    "\n",
    "```python\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Handling Connection Errors\n",
    "\n",
    "Wrap your request in a `try-except` block to handle errors gracefully:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "    print(response.text)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Best Practices for Fetching Pages\n",
    "\n",
    "‚úÖ Always check the HTTP status code.\n",
    "üß† Use proper headers to mimic a browser.\n",
    "‚è±Ô∏è Set a timeout to avoid hanging indefinitely.\n",
    "ü§ù Respect the website by not making too many rapid requests.\n",
    "\n",
    "---\n",
    "\n",
    "## üîö Summary\n",
    "\n",
    "* `requests` makes it **simple to fetch web pages** using Python.\n",
    "* It is the **starting point** for most web scraping workflows.\n",
    "* Combining `requests` with **BeautifulSoup** allows for **powerful data extraction**. üïµÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "---\n",
    "\n",
    "üîó Happy Scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7423d6e7-49d2-4e5c-ac04-46e0a67c6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa6de35-cc35-4ff4-93f9-2d2dddeff768",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = requests.get(\"https://books.toscrape.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d4afda-b46c-4d12-850c-ef4df3b035f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded page 1 successfully.\n",
      "Downloaded page 2 successfully.\n",
      "Downloaded page 3 successfully.\n",
      "Downloaded page 4 successfully.\n",
      "Downloaded page 5 successfully.\n",
      "Downloaded page 6 successfully.\n",
      "Downloaded page 7 successfully.\n",
      "Downloaded page 8 successfully.\n",
      "Downloaded page 9 successfully.\n",
      "Downloaded page 10 successfully.\n",
      "Downloaded page 11 successfully.\n",
      "Downloaded page 12 successfully.\n",
      "Downloaded page 13 successfully.\n",
      "Downloaded page 14 successfully.\n",
      "Downloaded page 15 successfully.\n",
      "Downloaded page 16 successfully.\n",
      "Downloaded page 17 successfully.\n",
      "Downloaded page 18 successfully.\n",
      "Downloaded page 19 successfully.\n",
      "Downloaded page 20 successfully.\n",
      "Downloaded page 21 successfully.\n",
      "Downloaded page 22 successfully.\n",
      "Downloaded page 23 successfully.\n",
      "Downloaded page 24 successfully.\n",
      "Downloaded page 25 successfully.\n",
      "Downloaded page 26 successfully.\n",
      "Downloaded page 27 successfully.\n",
      "Downloaded page 28 successfully.\n",
      "Downloaded page 29 successfully.\n",
      "Downloaded page 30 successfully.\n",
      "Downloaded page 31 successfully.\n",
      "Downloaded page 32 successfully.\n",
      "Downloaded page 33 successfully.\n",
      "Downloaded page 34 successfully.\n",
      "Downloaded page 35 successfully.\n",
      "Downloaded page 36 successfully.\n",
      "Downloaded page 37 successfully.\n",
      "Downloaded page 38 successfully.\n",
      "Downloaded page 39 successfully.\n",
      "Downloaded page 40 successfully.\n",
      "Downloaded page 41 successfully.\n",
      "Downloaded page 42 successfully.\n",
      "Downloaded page 43 successfully.\n",
      "Downloaded page 44 successfully.\n",
      "Downloaded page 45 successfully.\n",
      "Downloaded page 46 successfully.\n",
      "Downloaded page 47 successfully.\n",
      "Downloaded page 48 successfully.\n",
      "Downloaded page 49 successfully.\n",
      "Downloaded page 50 successfully.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 51):\n",
    "    a = requests.get(f\"https://books.toscrape.com/catalogue/page-{i}.html\")\n",
    "    with open(f'htmls/page-{i}.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(a.text)\n",
    "        print(f'Downloaded page {i} successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
